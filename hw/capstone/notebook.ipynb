{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a7975d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd83a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data() -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Import data\n",
    "    df_full_train = pd.read_csv('../../data/capstone/Video_games_esrb_rating.csv')\n",
    "    df_test = pd.read_csv('../../data/capstone/test_esrb.csv')\n",
    "\n",
    "    # Clean data\n",
    "    df_full_train = df_full_train.rename(columns={'strong_janguage': 'strong_language'})\n",
    "    df_full_train = df_full_train.drop('title', axis=1)\n",
    "    df_test = df_test.rename(columns={'strong_janguage': 'strong_language'})\n",
    "    df_test = df_test.drop('title', axis=1)\n",
    "    \n",
    "    return df_full_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b4ce61c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='esrb_rating', ylabel='Count'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "T     0.363588\n",
       "E     0.219525\n",
       "ET    0.212665\n",
       "M     0.204222\n",
       "Name: esrb_rating, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong_language</td>\n",
       "      <td>0.154571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_descriptors</td>\n",
       "      <td>0.145806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blood</td>\n",
       "      <td>0.088982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fantasy_violence</td>\n",
       "      <td>0.078090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blood_and_gore</td>\n",
       "      <td>0.070759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mild_fantasy_violence</td>\n",
       "      <td>0.047127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>suggestive_themes</td>\n",
       "      <td>0.037943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>language</td>\n",
       "      <td>0.034657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>violence</td>\n",
       "      <td>0.027594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>strong_sexual_content</td>\n",
       "      <td>0.025579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intense_violence</td>\n",
       "      <td>0.024174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mild_blood</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>alcohol_reference</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>simulated_gambling</td>\n",
       "      <td>0.021640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>console</td>\n",
       "      <td>0.020481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>crude_humor</td>\n",
       "      <td>0.019232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sexual_themes</td>\n",
       "      <td>0.018970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mild_violence</td>\n",
       "      <td>0.016529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mild_suggestive_themes</td>\n",
       "      <td>0.016440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mild_lyrics</td>\n",
       "      <td>0.014508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>lyrics</td>\n",
       "      <td>0.012048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mild_cartoon_violence</td>\n",
       "      <td>0.011759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mild_language</td>\n",
       "      <td>0.011203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>drug_reference</td>\n",
       "      <td>0.008363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>cartoon_violence</td>\n",
       "      <td>0.008186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sexual_content</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>use_of_alcohol</td>\n",
       "      <td>0.007452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>use_of_drugs_and_alcohol</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>animated_blood</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mature_humor</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>partial_nudity</td>\n",
       "      <td>0.004092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>nudity</td>\n",
       "      <td>0.003683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "0            strong_language    0.154571\n",
       "1             no_descriptors    0.145806\n",
       "2                      blood    0.088982\n",
       "3           fantasy_violence    0.078090\n",
       "4             blood_and_gore    0.070759\n",
       "5      mild_fantasy_violence    0.047127\n",
       "6          suggestive_themes    0.037943\n",
       "7                   language    0.034657\n",
       "8                   violence    0.027594\n",
       "9      strong_sexual_content    0.025579\n",
       "10          intense_violence    0.024174\n",
       "11                mild_blood    0.023579\n",
       "12         alcohol_reference    0.022100\n",
       "13        simulated_gambling    0.021640\n",
       "14                   console    0.020481\n",
       "15               crude_humor    0.019232\n",
       "16             sexual_themes    0.018970\n",
       "17             mild_violence    0.016529\n",
       "18    mild_suggestive_themes    0.016440\n",
       "19               mild_lyrics    0.014508\n",
       "20                    lyrics    0.012048\n",
       "21     mild_cartoon_violence    0.011759\n",
       "22             mild_language    0.011203\n",
       "23            drug_reference    0.008363\n",
       "24          cartoon_violence    0.008186\n",
       "25            sexual_content    0.007574\n",
       "26            use_of_alcohol    0.007452\n",
       "27  use_of_drugs_and_alcohol    0.006479\n",
       "28            animated_blood    0.005700\n",
       "29              mature_humor    0.004700\n",
       "30            partial_nudity    0.004092\n",
       "31                    nudity    0.003683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################\n",
    "# Exploratory Data Analysis #\n",
    "#############################\n",
    "\n",
    "df_full_train, df_test = get_data()\n",
    "\n",
    "target = 'esrb_rating'\n",
    "features = [col for col in df_full_train if col != target]\n",
    "\n",
    "for col in df_full_train:\n",
    "    \n",
    "    # Validate feature values as binary (checks min/max and range of values)\n",
    "    if col in features:\n",
    "        if set(df_full_train[col].unique()) != {0, 1}:\n",
    "            warnings.warn(f\"The feature {col} is not binary.\")\n",
    "            \n",
    "    # Check for missing values\n",
    "    if df_full_train[col].isnull().any():\n",
    "        warnings.warn(f\"The {col} column has missing values.\")\n",
    "            \n",
    "    # Analyze target variable\n",
    "    if col == 'esrb_rating':\n",
    "        # Note: As described in README.md, we're missing RP, EC, and A ratings\n",
    "        # The distribution of `esrb_rating` and histogram reveal we have a\n",
    "        # Multi-Class Imbalanced Classification problem where the ESRB Rating\n",
    "        # of T (Teen) is the Majority Class\n",
    "        display(sns.histplot(df_full_train[col]))  # TODO: Not sure why this is displayed last\n",
    "        distribution = df_full_train[col].value_counts() / len(df_full_train)\n",
    "        display(distribution)\n",
    "        \n",
    "# TODO: What about `no_descriptors`?\n",
    "\n",
    "# Feature importance analysis (using RandomForestClassifier)\n",
    "# https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "# TODO: use CART and XGBoost too\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "  n_estimators=10,\n",
    "  max_depth=20,\n",
    "  random_state=1,\n",
    "  n_jobs=-1,\n",
    ")\n",
    "rf.fit(df_full_train[features], df_full_train[target])\n",
    "\n",
    "feature_rank = (\n",
    "  pd\n",
    "  .DataFrame(\n",
    "    data=zip(features, rf.feature_importances_),\n",
    "    columns=['feature', 'importance'],\n",
    "  )\n",
    "  .sort_values('importance', ascending=False)\n",
    "  .reset_index(drop=True)\n",
    ")\n",
    "display(feature_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1a9606fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Split data #\n",
    "##############\n",
    "\n",
    "if target not in df_full_train:\n",
    "    # Cell has been run before, so grab data again for idempotency\n",
    "    df_full_train, df_test = get_data()\n",
    "    \n",
    "# Split full_train data into train/validation    \n",
    "df_train, df_val = train_test_split(\n",
    "    df_full_train,\n",
    "    test_size=0.25,\n",
    "    random_state=1,\n",
    ")\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# Extract target\n",
    "y_full_train = df_full_train.pop(target).values\n",
    "y_train = df_train.pop(target).values\n",
    "y_val = df_val.pop(target).values\n",
    "y_test = df_test.pop(target).values\n",
    "\n",
    "# Set features (might be reduced based on FeatureImportance)\n",
    "features = [col for col in df_full_train if col != target]\n",
    "\n",
    "# Prepare data\n",
    "train_dicts = df_train[features].to_dict(orient='records')\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[features].to_dict(orient='records')\n",
    "X_val = dv.fit_transform(val_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "12300f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:45:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoffrey/.local/share/virtualenvs/ml-Yx4f4dZ2/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:45:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "            model_name                        params     score\n",
      "21  LogisticRegression        None, ovr, none, lbfgs  0.843882\n",
      "20  LogisticRegression    None, ovr, none, newton-cg  0.843882\n",
      "9   LogisticRegression    None, ovr, none, newton-cg  0.843882\n",
      "10  LogisticRegression        None, ovr, none, lbfgs  0.843882\n",
      "23  LogisticRegression         None, ovr, none, saga  0.839662\n",
      "..                 ...                           ...       ...\n",
      "38  LogisticRegression      balanced, ovr, l2, lbfgs  0.805907\n",
      "26  LogisticRegression  balanced, ovr, l2, newton-cg  0.805907\n",
      "67                 SVM              balanced sigmoid  0.774262\n",
      "63                 SVM                  None sigmoid  0.727848\n",
      "1           GaussianNB                                0.594937\n",
      "\n",
      "[70 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Model Selection and Tuning #\n",
    "##############################\n",
    "\n",
    "# Try the following models with various parameters:\n",
    "#   o NaiveBayes: https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/\n",
    "#   o LogisticRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#   o DecisionTreeClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "#   o RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#   o SVM: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "#   o XGBoost: https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "\n",
    "# Note 1: For convenience, we go ahead and try multiple parameters per model\n",
    "#   in order to simultaneously train the best model and tune its parameters.\n",
    "\n",
    "# Note 2: The following code will likely throw some Warnings, but it is safe\n",
    "#   to ignore them in favor of working models (which will be displayed in the\n",
    "#   `scores` DataFrame at the end).\n",
    "\n",
    "scores = []\n",
    "\n",
    "model_name = 'BernoulliNB'  # NativeBayes\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_val, y_val)\n",
    "\n",
    "params = \"\"\n",
    "stats = [model_name, params, score]\n",
    "scores.append(stats)\n",
    "\n",
    "\n",
    "model_name = 'GaussianNB'  # NativeBayes\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_val, y_val)\n",
    "\n",
    "params = \"\"\n",
    "stats = [model_name, params, score]\n",
    "scores.append(stats)\n",
    "\n",
    "# TODO: CLASS_WEIGHT\n",
    "model_name = 'LogisticRegression'\n",
    "for class_weight in [None, 'balanced']:\n",
    "    for multi_classs in ['multinomial', 'ovr']:\n",
    "        for penalty in ['l1', 'l2', 'elasticnet', 'none']:\n",
    "            for solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "                try:\n",
    "                    model = LogisticRegression(\n",
    "                        random_state=0,\n",
    "                        class_weight=class_weight,\n",
    "                        multi_class=multi_class,\n",
    "                        penalty=penalty,\n",
    "                        solver=solver,\n",
    "                    )\n",
    "                    model = model.fit(X_train, y_train)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "                else:\n",
    "                    score = model.score(X_val, y_val)\n",
    "\n",
    "                    params = f\"{class_weight}, {multi_class}, {penalty}, {solver}\"\n",
    "                    stats = [model_name, params, score]\n",
    "                    scores.append(stats)\n",
    "\n",
    "                \n",
    "# TODO: CLASS_WEIGHT\n",
    "model_name = 'DecisionTreeClassifier'\n",
    "for class_weight in [None, 'balanced']:\n",
    "    for criterion in ['gini', 'entropy']:\n",
    "        for splitter in ['best', 'random']:\n",
    "            try:\n",
    "                model = DecisionTreeClassifier(\n",
    "                    random_state=0,\n",
    "                    class_weight=class_weight,\n",
    "                    criterion=criterion,\n",
    "                    splitter=splitter,\n",
    "                )\n",
    "                model = model.fit(X_train, y_train)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            else:\n",
    "                score = model.score(X_val, y_val)\n",
    "\n",
    "                params = f\"{class_weight}, {criterion}, {splitter}\"\n",
    "                stats = [model_name, params, score]\n",
    "                scores.append(stats)\n",
    "\n",
    "\n",
    "# TODO: CLASS_WEIGHT\n",
    "model_name = 'RandomForestClassifier'\n",
    "for class_weight in [None, 'balanced', 'balanced_subsample']:\n",
    "    for criterion in ['gini', 'entropy']:\n",
    "        try:\n",
    "            model = RandomForestClassifier(\n",
    "                random_state=1,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "            model = model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else:\n",
    "            score = model.score(X_val, y_val)\n",
    "\n",
    "            params = f\"{class_weight}, {criterion}\"\n",
    "            stats = [model_name, params, score]\n",
    "            scores.append(stats)\n",
    "\n",
    "        \n",
    "model_name = 'SVM'\n",
    "for class_weight in [None, 'balanced']:\n",
    "    for kernal in ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']:\n",
    "        try:\n",
    "            model = SVC(\n",
    "                random_state=1,\n",
    "                class_weight=class_weight,\n",
    "                kernel=kernal,\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        else:\n",
    "            score = model.score(X_val, y_val)\n",
    "\n",
    "            params = f\"{class_weight} {kernal}\"\n",
    "            stats = [model_name, params, score]\n",
    "            scores.append(stats)\n",
    "\n",
    "\n",
    "model_name = 'XGBoost'\n",
    "for objective in ['binary:logistic', 'multi:softmax']:\n",
    "    model = XGBClassifier(\n",
    "        random_state=1,\n",
    "        objective=objective\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_val, y_val)\n",
    "\n",
    "    params = f\"{objective}\"\n",
    "    stats = [model_name, params, score]\n",
    "    scores.append(stats)\n",
    "\n",
    "# Rank models by score\n",
    "scores = pd.DataFrame(\n",
    "    data=scores,\n",
    "    columns=['model_name', 'params', 'score']\n",
    ")\n",
    "scores = scores.sort_values('score', ascending=False)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e6693c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictVectorizer saved to ../../servers/capstone/dv.bin\n",
      "Model saved to ../../servers/capstone/model.bin\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Export model to script #\n",
    "##########################\n",
    "\n",
    "# Note: This is also done in hw/capstone/train.py\n",
    "#       If you want to run that script, make sure you're in the\n",
    "#       proper directory, as specified in the README.md\n",
    "\n",
    "\n",
    "# Convert df_full_train\n",
    "val_dicts = df_full_train[features].to_dict(orient='records')\n",
    "X_full_train = dv.fit_transform(val_dicts)\n",
    "\n",
    "# Use LogisticRegression model with One-Vs-Rest, no penalty, lbfgs solver\n",
    "model = LogisticRegression(\n",
    "    random_state=0,\n",
    "    class_weight=None,\n",
    "    multi_class='ovr',\n",
    "    penalty='none',\n",
    "    solver='newton-cg',\n",
    ")\n",
    "model.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Pickle model files into binaries\n",
    "filepath = os.path.join('../../servers/capstone/', 'dv.bin')\n",
    "with open(filepath, 'wb') as f:\n",
    "  pickle.dump(dv, f)\n",
    "print(f'DictVectorizer saved to {filepath}')\n",
    "\n",
    "filepath = os.path.join('../../servers/capstone', 'model.bin')\n",
    "with open(filepath, 'wb') as f:\n",
    "  pickle.dump(model, f)\n",
    "print(f'Model saved to {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dc97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
